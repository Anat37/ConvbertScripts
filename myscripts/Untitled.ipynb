{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AlbertConfig, AlbertTokenizer, AlbertForPreTraining, ConvbertForPreTraining\n",
    "from dataset import SOPDataset, MyTrainer, collate_batch\n",
    "import os\n",
    "\n",
    "model_dir = 'E:/ConvbertData/convbert/output'\n",
    "#model_dir = 'D:/ConvbertData/albert_model_dir'\n",
    "\n",
    "def get_last_checkpoint(dir_name):\n",
    "    max_check = -1\n",
    "    result = None\n",
    "    for filename in os.listdir(dir_name):\n",
    "        if 'checkpoint' in filename:\n",
    "            step = int(filename.split('-')[1])\n",
    "            if step > max_check:\n",
    "                max_check = step\n",
    "                result = filename\n",
    "    return os.path.join(dir_name, result)\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = ConvbertForPreTraining.from_pretrained(get_last_checkpoint(model_dir))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "train_dataset = SOPDataset(directory='E:/ConvbertData/text_data/cache', batch_size=1, tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   14, 2008,  847,   27,   14, 4277,    3]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"The cat sat on the mat\", return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0., -0., -0., -0., -0., -0., -0., -0.]]], device='cuda:0')\n",
      "tensor([-10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000.],\n",
      "       device='cuda:0')\n",
      "3\n",
      "torch.Size([1, 8, 30000])\n",
      "torch.Size([1, 2])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs.to(device), output_attentions=True)\n",
    "print(len(outputs))\n",
    "print(outputs[0].shape)\n",
    "print(outputs[1].shape)\n",
    "print(len(outputs[2]))\n",
    "attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.3712, 0.0270, 0.1055, 0.0880, 0.0889, 0.1682, 0.1165, 0.0348,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0].view(12, 8, 255)[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[[-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "          -0., -0., -0., -0., -0., -0.]]], device='cuda:0')\n",
      "tensor([-10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "        -10000., -10000., -10000., -10000., -10000., -10000., -10000.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "            -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch in iter(train_dataset):\n",
    "    print(batch.data['attention_mask'])\n",
    "    outputs = model(**batch.to(device))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit",
   "language": "python",
   "name": "python36564bit5f07781c00224688be8c373277587870"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
